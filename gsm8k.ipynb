{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLLM Installation, Using Llama-3.2-1B as source model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-07 23:18:26 config.py:510] This model supports multiple tasks: {'classify', 'generate', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 01-07 23:18:26 arg_utils.py:1103] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 01-07 23:18:26 config.py:1458] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 01-07 23:18:26 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='meta-llama/Llama-3.2-1B', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-1B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-07 23:18:27 model_runner.py:1094] Starting to load model meta-llama/Llama-3.2-1B...\n",
      "INFO 01-07 23:18:27 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "INFO 01-07 23:18:27 weight_utils.py:296] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.76it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-07 23:18:28 model_runner.py:1099] Loading model weights took 2.3029 GB\n",
      "INFO 01-07 23:18:28 model_runner_base.py:120] Writing input of failed execution to /tmp/err_execute_model_input_20250107-231828.pkl...\n",
      "INFO 01-07 23:18:28 model_runner_base.py:149] Completed writing input of failed execution to /tmp/err_execute_model_input_20250107-231828.pkl.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Error in model execution (input dumped to /tmp/err_execute_model_input_20250107-231828.pkl): CUDA out of memory. Tried to allocate 252.00 MiB. GPU 0 has a total capacity of 21.98 GiB of which 98.44 MiB is free. Including non-PyTorch memory, this process has 21.86 GiB memory in use. Of the allocated memory 21.42 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 71.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py:116\u001b[0m, in \u001b[0;36mdump_input_when_exception.<locals>._inner.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/vllm/worker/model_runner.py:1747\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, model_input, kv_caches, intermediate_tensors, num_steps)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[0;32m-> 1747\u001b[0m output: SamplerOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config\u001b[38;5;241m.\u001b[39mcollect_model_forward_time\n\u001b[1;32m   1753\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py:584\u001b[0m, in \u001b[0;36mLlamaForCausalLM.sample\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, logits: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    583\u001b[0m            sampling_metadata: SamplingMetadata) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[SamplerOutput]:\n\u001b[0;32m--> 584\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:274\u001b[0m, in \u001b[0;36mSampler.forward\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_top_p_top_k \u001b[38;5;129;01mand\u001b[39;00m flashinfer_top_k_top_p_sampling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43m_apply_top_k_top_p\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_ps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                                \u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_ks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_min_p:\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py:392\u001b[0m, in \u001b[0;36m_apply_top_k_top_p\u001b[0;34m(logits, p, k)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply_top_k_top_p\u001b[39m(\n\u001b[1;32m    388\u001b[0m     logits: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    389\u001b[0m     p: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    390\u001b[0m     k: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 392\u001b[0m     logits_sort, logits_idx \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;66;03m# Apply top-k.\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 252.00 MiB. GPU 0 has a total capacity of 21.98 GiB of which 98.44 MiB is free. Including non-PyTorch memory, this process has 21.86 GiB memory in use. Of the allocated memory 21.42 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 71.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-1B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/vllm/utils.py:986\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    979\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    981\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    982\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m    983\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m    984\u001b[0m         )\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/vllm/entrypoints/llm.py:230\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# TODO(rob): enable mp by default (issue with fork vs spawn)\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py:517\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    515\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py:276\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m executor_class(vllm_config\u001b[38;5;241m=\u001b[39mvllm_config, )\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mrunner_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_kv_caches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py:416\u001b[0m, in \u001b[0;36mLLMEngine._initialize_kv_caches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the KV cache in the worker(s).\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03mThe workers will determine the number of blocks in both the GPU cache\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;124;03mand the swap CPU cache.\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    414\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    415\u001b[0m num_gpu_blocks, num_cpu_blocks \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetermine_num_available_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks_override \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m     num_gpu_blocks_override \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks_override\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py:68\u001b[0m, in \u001b[0;36mGPUExecutor.determine_num_available_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdetermine_num_available_blocks\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Determine the number of available KV blocks by invoking the\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    underlying worker.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetermine_num_available_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/vllm/worker/worker.py:202\u001b[0m, in \u001b[0;36mWorker.determine_num_available_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Execute a forward pass with dummy inputs to profile the memory usage\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# of the model.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m memory_profiling(baseline_memory_in_bytes\u001b[38;5;241m=\u001b[39mtotal_gpu_memory \u001b[38;5;241m-\u001b[39m\n\u001b[1;32m    199\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_gpu_memory,\n\u001b[1;32m    200\u001b[0m                       weights_memory_in_bytes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_runner\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    201\u001b[0m                       model_memory_usage) \u001b[38;5;28;01mas\u001b[39;00m result:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofile_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_memory_footprint_increased_during_profiling()\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/vllm/worker/model_runner.py:1331\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.profile_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_first_rank:\n\u001b[1;32m   1326\u001b[0m     intermediate_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmake_empty_intermediate_tensors(\n\u001b[1;32m   1327\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1328\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m   1329\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1332\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/multiagent_debate/.venv/lib/python3.10/site-packages/vllm/worker/model_runner_base.py:152\u001b[0m, in \u001b[0;36mdump_input_when_exception.<locals>._inner.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(err)(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in model execution: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(err)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    149\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted writing input of failed execution to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    151\u001b[0m         filename)\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(err)(\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in model execution (input dumped to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(err)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Error in model execution (input dumped to /tmp/err_execute_model_input_20250107-231828.pkl): CUDA out of memory. Tried to allocate 252.00 MiB. GPU 0 has a total capacity of 21.98 GiB of which 98.44 MiB is free. Including non-PyTorch memory, this process has 21.86 GiB memory in use. Of the allocated memory 21.42 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 71.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = LLM(model = model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion(llm_engine, messages, max_length=1000, temperature=0.7):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "        \n",
    "        if role == \"system\":\n",
    "            prompt += f\"System: {content}\\n\"\n",
    "        elif role == \"user\":\n",
    "            prompt += f\"User: {content}\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            prompt += f\"Assistant: {content}\\n\"\n",
    "    \n",
    "    prompt += \"Assistant: \"\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_length,\n",
    "    )\n",
    "    \n",
    "    outputs = llm_engine.generate([prompt], sampling_params)\n",
    "    generated_text = outputs[0].outputs[0].text\n",
    "    \n",
    "    completion = {\n",
    "        \"choices\": [{\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": generated_text.strip()\n",
    "            },\n",
    "            \"finish_reason\": \"stop\"\n",
    "        }],\n",
    "    }\n",
    "    \n",
    "    return completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Agent Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_message(agents, question, idx):\n",
    "    if len(agents) == 0:\n",
    "        return {\"role\": \"user\", \"content\": \"Can you double check that your answer is correct. Please reiterate your answer, with your final answer a single numerical number, in the form \\\\boxed{{answer}}.\"}\n",
    "\n",
    "    prefix_string = \"These are the solutions to the problem from other agents: \"\n",
    "\n",
    "    for agent in agents:\n",
    "        agent_response = agent[idx][\"content\"]\n",
    "        response = \"\\n\\n One agent solution: ```{}```\".format(agent_response)\n",
    "\n",
    "        prefix_string = prefix_string + response\n",
    "\n",
    "    prefix_string = prefix_string + \"\"\"\\n\\n Using the solutions from other agents as additional information, can you provide your answer to the math problem? \\n The original math problem is {}. Your final answer should be a single numerical number, in the form \\\\boxed{{answer}}, at the end of your response.\"\"\".format(question)\n",
    "    return {\"role\": \"user\", \"content\": prefix_string}\n",
    "\n",
    "\n",
    "def construct_assistant_message(completion):\n",
    "    content = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return {\"role\": \"assistant\", \"content\": content}\n",
    "\n",
    "\n",
    "def read_jsonl(path: str):\n",
    "    with open(path) as fh:\n",
    "        return [json.loads(line) for line in fh.readlines() if line]\n",
    "\n",
    "\n",
    "def run_generation(agents = 1, rounds = 1, seed = 0):\n",
    "    random.seed(seed)\n",
    "\n",
    "    generated_description = {}\n",
    "\n",
    "    questions = read_jsonl(\"/home/ubuntu/multiagent_debate/grade-school-math/grade_school_math/data/test.jsonl\")\n",
    "    random.shuffle(questions)\n",
    "\n",
    "    for data in questions[:100]:\n",
    "        question = data['question']\n",
    "        answer = data['answer']\n",
    "\n",
    "        agent_contexts = [[{\"role\": \"user\", \"content\": \"\"\"Can you solve the following math problem? {} Explain your reasoning. Your final answer should be a single numerical number, in the form \\\\boxed{{answer}}, at the end of your response. \"\"\".format(question)}] for agent in range(agents)]\n",
    "\n",
    "        for round in range(rounds):        \n",
    "            for i, agent_context in enumerate(agent_contexts):\n",
    "                if round != 0:\n",
    "                    agent_contexts_other = agent_contexts[:i] + agent_contexts[i+1:]\n",
    "                    message = construct_message(agent_contexts_other, question, 2*round - 1)\n",
    "                    agent_context.append(message)\n",
    "\n",
    "                completion = generate_completion(model, agent_context, temperature = 0.1)\n",
    "\n",
    "                assistant_message = construct_assistant_message(completion)\n",
    "                agent_context.append(assistant_message)\n",
    "\n",
    "        generated_description[question] = (agent_contexts, answer)\n",
    "        break\n",
    "\n",
    "    json.dump(generated_description, open(\"gen_data/gsm_{}_{}.json\".format(agents, rounds), \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it, est. speed input: 169.26 toks/s, output: 280.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: 1, Rounds: 1, Time Taken: 7153.552055358887 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.15s/it, est. speed input: 11.89 toks/s, output: 139.87 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.23s/it, est. speed input: 155.57 toks/s, output: 138.28 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: 1, Rounds: 2, Time Taken: 14397.95708656311 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.16s/it, est. speed input: 11.88 toks/s, output: 139.78 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.26s/it, est. speed input: 154.96 toks/s, output: 137.74 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.42s/it, est. speed input: 291.76 toks/s, output: 134.76 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: 1, Rounds: 3, Time Taken: 21861.364364624023 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.18s/it, est. speed input: 11.84 toks/s, output: 139.32 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.25s/it, est. speed input: 155.04 toks/s, output: 138.05 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.42s/it, est. speed input: 291.31 toks/s, output: 134.80 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.54s/it, est. speed input: 424.42 toks/s, output: 132.67 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: 1, Rounds: 4, Time Taken: 29412.036418914795 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.20s/it, est. speed input: 11.81 toks/s, output: 138.99 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.31s/it, est. speed input: 11.63 toks/s, output: 136.84 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: 2, Rounds: 1, Time Taken: 14518.091917037964 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.30s/it, est. speed input: 11.65 toks/s, output: 137.05 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.39s/it, est. speed input: 11.51 toks/s, output: 135.39 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.54s/it, est. speed input: 291.95 toks/s, output: 132.64 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.57s/it, est. speed input: 290.78 toks/s, output: 132.05 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: 2, Rounds: 2, Time Taken: 29826.023817062378 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.40s/it, est. speed input: 11.49 toks/s, output: 135.23 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.33s/it, est. speed input: 11.60 toks/s, output: 136.48 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.57s/it, est. speed input: 291.13 toks/s, output: 132.21 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.63s/it, est. speed input: 288.56 toks/s, output: 131.04 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.93s/it, est. speed input: 545.13 toks/s, output: 126.22 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.78s/it, est. speed input: 555.07 toks/s, output: 128.52 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: 2, Rounds: 3, Time Taken: 45677.77991294861 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.33s/it, est. speed input: 11.59 toks/s, output: 136.37 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.49s/it, est. speed input: 57.00 toks/s, output: 138.15 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.36s/it, est. speed input: 190.99 toks/s, output: 135.84 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.37s/it, est. speed input: 190.87 toks/s, output: 135.66 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.59s/it, est. speed input: 464.35 toks/s, output: 131.84 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.63s/it, est. speed input: 461.92 toks/s, output: 131.12 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.98s/it, est. speed input: 706.69 toks/s, output: 125.34 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.92s/it, est. speed input: 712.34 toks/s, output: 126.32 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: 2, Rounds: 4, Time Taken: 54736.926317214966 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.91s/it, est. speed input: 14.39 toks/s, output: 138.65 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.28s/it, est. speed input: 11.68 toks/s, output: 137.42 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.21s/it, est. speed input: 11.80 toks/s, output: 138.82 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: 3, Rounds: 1, Time Taken: 20405.33757209778 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.23s/it, est. speed input: 11.76 toks/s, output: 138.33 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.20s/it, est. speed input: 11.80 toks/s, output: 138.85 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.21s/it, est. speed input: 11.79 toks/s, output: 138.72 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.65s/it, est. speed input: 419.50 toks/s, output: 130.81 toks/s]\n"
     ]
    }
   ],
   "source": [
    "for agent_num in range(1, 8):\n",
    "    for rounds in range(1, 5):\n",
    "        start = time.time()\n",
    "        run_generation(agent_num, rounds)\n",
    "        \n",
    "        print(f\"Agents: {agent_num}, Rounds: {rounds}, Time Taken: {1000 * (time.time() - start)} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Generated Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bullets(sentence):\n",
    "    bullets_preprocess = sentence.split(\"\\n\")\n",
    "    bullets = []\n",
    "\n",
    "    for bullet in bullets_preprocess:\n",
    "        try:\n",
    "            idx = bullet.find(next(filter(str.isalpha, bullet)))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        bullet = bullet[idx:]\n",
    "\n",
    "        if len(bullet) != 0:\n",
    "            bullets.append(bullet)\n",
    "\n",
    "    return bullets\n",
    "\n",
    "def parse_yes_no(string):\n",
    "    if \"yes\" in string.lower():\n",
    "        return True\n",
    "    elif \"no\" in string.lower():\n",
    "        return False\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def solve_math_problems(input_str):\n",
    "    pattern = r\"\\d+\\.?\\d*\"\n",
    "\n",
    "    matches = re.findall(pattern, input_str)\n",
    "    if matches:\n",
    "        return matches[-1]\n",
    "\n",
    "    return None\n",
    "\n",
    "def parse_answer(input_str):\n",
    "    pattern = r\"\\{([0-9.,$]*)\\}\"\n",
    "    matches = re.findall(pattern, input_str)\n",
    "\n",
    "    solution = None\n",
    "\n",
    "    for match_str in matches[::-1]:\n",
    "        solution = re.sub(r\"[^0-9.]\", \"\", match_str)\n",
    "        if solution:\n",
    "            break\n",
    "\n",
    "    return solution\n",
    "\n",
    "def compute_accuracy(gt, pred_solution):\n",
    "    answers = solve_math_problems(gt)\n",
    "\n",
    "    if answers is None:\n",
    "        return None\n",
    "\n",
    "    if type(pred_solution) == list:\n",
    "        pred_answers = []\n",
    "\n",
    "        for pred_solution in pred_solutions:\n",
    "            pred_answer = parse_answer(pred_solution)\n",
    "\n",
    "            if pred_answer is None:\n",
    "                pred_answer = solve_math_problems(pred_solution)\n",
    "\n",
    "            pred_answers.append(pred_answer)\n",
    "\n",
    "        pred_answer = most_frequent(pred_answers)\n",
    "\n",
    "    else:\n",
    "        pred_answer = parse_answer(pred_solution)\n",
    "        if pred_answer is None:\n",
    "            pred_answer = solve_math_problems(pred_solution)\n",
    "\n",
    "    if pred_answer is None:\n",
    "        return 1\n",
    "\n",
    "    if float(answers) == float(pred_answer):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def most_frequent(List):\n",
    "    counter = 0\n",
    "    num = List[0]\n",
    "\n",
    "    for i in List:\n",
    "        current_frequency = List.count(i)\n",
    "        if current_frequency > counter:\n",
    "            counter = current_frequency\n",
    "            num = i\n",
    "\n",
    "    return num\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    response_dict = json.load(open(\"gsm_debate_3_3.json\", \"r\"))\n",
    "\n",
    "    questions = list(response_dict.keys())\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    for question in questions:\n",
    "        responses, gt = response_dict[question]\n",
    "\n",
    "        pred_solutions = []\n",
    "        for response in responses:\n",
    "            pred_solution = response[-1]['content']\n",
    "\n",
    "            pred_solutions.append(pred_solution)\n",
    "\n",
    "        accurate = compute_accuracy(gt, pred_solutions)\n",
    "\n",
    "        if accurate is not None:\n",
    "            accuracies.append(float(accurate))\n",
    "        else:\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            print(gt)\n",
    "\n",
    "        print(\"accuracies:\", np.mean(accuracies), np.std(accuracies) / (len(accuracies) ** 0.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
